# -*- coding: utf-8 -*-
"""Query_PDF(Cosine)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFwA1SEzQtzZZm_DC6V4dtndk2hwlFfk

# **Rough code**

Importing libraries and extraction of images
"""

# import fitz   #from PyMuPDF
# import PIL.Image   #from pillow
# import io
# pdf = fitz.open("/content/Aekiusoft Proposal.pdf")
# counter = 1 #a counter to keep track of the number of images
# for i in range(len(pdf)):
#     page = pdf[i]
#     images = page.get_images()
#     for image in images:
#         base_img = pdf.extract_image(image[0])
#         image_data = base_img["image"]
#         img = PIL.Image.open(io.BytesIO(image_data))
#         extension = base_img["ext"] #extension of the image
#         img.save(open(f"image{counter}.{extension}","wb"))
#         counter += 1

# import tabula
# tables = tabula.read_pdf("/content/Aekiusoft Proposal.pdf",pages = "all")
# df = tables[0]
# print(df)

# !pip install transformers

# from transformers import BertTokenizer, BertModel
# import torch

# #defining a func to call for each iteration
# def get_bert_embeddings(text, tokenizer, model, max_length=512):
#     inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)
#     with torch.no_grad():
#         outputs = model(**inputs)
#     return outputs.last_hidden_state

# #  using iterations to tokenise long text by splitting it into chunks as bert has limit of 512
# long_text = text
# chunks = [long_text[i:i+512] for i in range(0, len(long_text), 512)]
# all_embeddings = []

# for chunk in chunks:
#     embeddings = get_bert_embeddings(chunk, tokenizer, model)
#     all_embeddings.append(embeddings)

# # Combine embeddings if needed
# combined_embeddings = torch.cat(all_embeddings, dim=1)  # Concatenate along the sequence dimension

# print(combined_embeddings)





"""# **Project Code**

**Roadmap for this project**

-Setup and Initialization

-Text Extraction from PDFs

-Compute Embeddings

-Convert Text to Vector

-Create FAISS Index

-Save and Load Embeddings

-Search for Similar Documents

-Process PDF Files

-Main script

First the script processes the PDFs in the specified directory, computes embeddings, and saves them to an HDF5 file.
Then, it enters an interactive loop where it prompts the user for queries, searches the FAISS index, and displays the results until the user types exit.

IMPORTING NEEDED LIBRARIES
"""

import os  # For file and directory operations
import h5py  # For handling HDF5 files
import numpy as np  # For numerical operations
import torch  # For PyTorch operations
import pdfplumber  # For extracting text from PDF files
from transformers import BertTokenizer, BertModel  # For using pre-trained BERT model and tokenizer
import faiss  # For efficient similarity search
import logging

"""Data extraction & creating embeddings"""

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:  # Check if text extraction was successful
                text += page_text
    return text

# Function to get combined embeddings for long texts
def get_combined_embeddings(text, tokenizer, model, max_length=512):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)
    input_ids = inputs['input_ids'][0]
    attention_mask = inputs['attention_mask'][0]

    # Calculate number of chunks needed
    num_chunks = (len(input_ids) + max_length - 1) // max_length
    all_embeddings = []

    for i in range(num_chunks):
        chunk_input_ids = input_ids[i*max_length:(i+1)*max_length].unsqueeze(0)
        chunk_attention_mask = attention_mask[i*max_length:(i+1)*max_length].unsqueeze(0)
        with torch.no_grad():
            outputs = model(chunk_input_ids, attention_mask=chunk_attention_mask)
        chunk_embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling of token embeddings
        all_embeddings.append(chunk_embeddings)

    combined_embeddings = torch.cat(all_embeddings, dim=0).mean(dim=0)  # Combine and average all chunk embeddings
    return combined_embeddings.cpu().numpy().reshape(1, -1)  # Reshape to 2D array

"""Text to Vector"""

# Function to convert text to vector
def text_to_vector(text, tokenizer, model):
    return get_combined_embeddings(text, tokenizer, model)

"""Functions for the folowing

*   Creating and searching FAISS index
*   Saving and Loading embeddings from HDF5


"""

# Function to create FAISS index and add embeddings
def create_faiss_index(embeddings):
    # Print the shape of the embeddings for debugging
    print(f"Embeddings shape: {embeddings.shape}")

    # Check if the embeddings array is empty
    if embeddings.size == 0:
        print("No embeddings provided.")
        return None

    # Ensure embeddings are 2-dimensional
    if len(embeddings.shape) != 2:
        print("Embeddings should be a 2-dimensional array (num_embeddings x embedding_dim).")
        return None

    d = embeddings.shape[1]  # Get the number of dimensions

    # Normalize embeddings to unit length for cosine similarity
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

    # Initialize Faiss index for Inner Product
    index = faiss.IndexFlatIP(d)

    # Add embeddings to the index (ensure they are float32 for Faiss)
    index.add(embeddings.astype(np.float32))

    return index

# Function to save embeddings and texts to HDF5
def save_to_hdf5(embeddings, text, pdf_path, filename='embeddings.h5'):
    with h5py.File(filename, 'w') as f:
        f.create_dataset('embeddings', data=embeddings)
        f.create_dataset('texts', data=np.string_(text.encode('utf-8')))
        f.create_dataset('pdf_path', data=np.string_(pdf_path))

# Function to load embeddings and text from HDF5
def load_from_hdf5(filename='embeddings.h5'):
    with h5py.File(filename, 'r') as f:
        embeddings = np.array(f['embeddings'])
        text = f['texts'][()].decode()
        pdf_path = f['pdf_path'][()].decode()
    index = create_faiss_index(embeddings)  # Recreate FAISS index
    return index, text, pdf_path

# Function to search the FAISS index
def search_index(query, index, tokenizer, model, k=3):
    query_vector = text_to_vector(query, tokenizer, model)
    query_vector = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)  # Normalize for cosine similarity
    D, I = index.search(query_vector.reshape(1, -1), k)  # Search for top-k nearest neighbors
    return D.flatten(), I.flatten().tolist()

"""Functions for answering the query"""

# Function to get user query
def get_user_query():
    return input("Ask a question: ")

# Function to formulate an answer from the retrieved data
def formulate_answer(query, retrieved_data, distances):
    answer = f"The answer to your query '{query}' is:\n"
    for i, item in enumerate(retrieved_data):
        similarity = distances[i]  # Since we are using Inner Product, the distances are already similarities
        answer += f"\t* Document {i + 1} (Similarity: {similarity:.2f}):\n"
        answer += f"\t\t- Text Snippet: {item[:200]}...\n"  # Display a snippet of the text
    return answer

"""Processing function of the program(for multiple PDFs)"""

# Main function to process a single PDF
def process_pdf(pdf_path, output_filename='embeddings.h5'):
    # Extract text from PDF
    text = extract_text_from_pdf(pdf_path)

    # Compute embeddings for the PDF text
    embeddings = get_combined_embeddings(text, tokenizer, model)

    # Save embeddings and corresponding text to HDF5 file
    save_to_hdf5(embeddings, text, pdf_path, output_filename)

    return embeddings, text, pdf_path

"""An example usage"""

# Example usage

#if __name__ == "__main__": is a Python idiom used to ensure that specific code runs only when the script is executed directly,
#not when it is imported as a module into another script.
if __name__ == "__main__":
    pdf_path = r"/content/StockReport_2016-07.pdf"  # Replace with your PDF file path

    # Process the single PDF
    embeddings, text, pdf_path = process_pdf(pdf_path)

    # Create FAISS index with the embeddings
    index = create_faiss_index(embeddings)

    # Get user query and search the index
    while True:
        query = get_user_query()
        if query.lower() == 'exit':
            break

        # Search for similar texts in the FAISS index
        distances, results = search_index(query, index, tokenizer, model)

        if results:
            retrieved_text = text  # For a single PDF, retrieved text is the same as the input text
            answer = f"The answer to your query '{query}' is:\n"
            for i, result in enumerate(results):
                answer += f"\t* Document {result + 1} (Similarity: {distances[i]:.2f}):\n"
                answer += f"\t\t- Text Snippet: {retrieved_text[:200]}...\n"  # Display a snippet of the text
            print(answer)
        else:
            print("No similar texts found.")